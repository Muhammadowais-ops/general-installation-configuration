####### rhel 9.5 i have use developer account

3 vms with one extra disk of 12 gb on each node


subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms
subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms
subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms
 
 
yum install cephadm

podman login registry.redhat.io
Username: xxxxxxxx
Password: xxxxxxxxxxxxx


cephadm install
cephadm bootstrap --mon-ip 172.30.140.52
 
 
cephadm shell
 
 
ceph -s
 
ceph orch host add ceph02 172.30.135.41
ceph orch host add ceph03 172.30.131.247


ceph orch host ls
ceph orch device ls

ceph osd tree

ceph orch daemon add osd ceph01:/dev/sdb

ceph osd tree


ceph orch daemon add osd ceph02:/dev/sdb
ceph orch daemon add osd ceph03:/dev/sdb



ceph orch ps  ---> check container status 
ceph orch ls --> check monitor node

ceph -s ---> check health 

### we are seeing error in ceph we have debug with below command and then fix it
ceph orch ps ---> show error on node exporter


ceph orch rm node-exporter               ---> removing
ceph orch apply node-exporter           ---> adding it again







##### pool creation #############
ceph osd pool create new1 

ceph osd pool create repool3 32 32 replicated

ceph osd pool set repool3 size 4

ceph osd pool rename repool3 repoolnew




ceph osd pool ls




####### enable delete for pool##############
ceph config set mon mon_allow_pool_delete true
ceph config get mon mon_allow_pool_delete



###########now delete##################
ceph osd pool delete repoolnew repoolnew --yes-i-really-really-mean-it



ceph osd erauser-code-profile ls


ceph osd erasure-code-profile set erause-profile-k2-m2 k=2 m=2


ceph osd erasure-code-profile set ec-profile-k2-m2 \
    plugin=jerasure \
    k=2 \
    m=2 \
    crush-failure-domain=osd

ceph osd erasure-code-profile get ec-profile-k2-m2


ceph osd pool create erasure-pool erasure ec-profile-k2-m2



##### ceph osd status ########
ceph osd status

ceph health detail
ceph osd dump
ceph df

#### ceph config #########
ceph config dump    
ceph config get mon.* 



########## ceph authentication ###############

ceph auth list      # List all keys and capabilities
ceph auth get client.admin

ceph auth get-or-create client.myuser mon 'allow r' osd 'allow rw pool=myreplicapool'




ceph auth get-or-create client.readonlyuser \
  mon 'allow r' \
  osd 'allow r pool=device_health_metrics'


ceph auth get client.readonlyuser -o /etc/ceph/ceph.client.readonlyuser.keyring


 ceph auth get client.readonlyuser


ceph --name client.readonlyuser --keyring /etc/ceph/ceph.client.readonlyuser.keyring \
  osd pool ls



ceph --name client.readonlyuser --keyring /etc/ceph/ceph.client.readonlyuser.keyring \
  osd pool stats device_health_metrics                                   ########## This user will only be able to read from the device_health_metrics pool and cannot write or access other pools.



########## cephfs ######################
ceph osd pool create cephfs_metadata 64

ceph osd pool create cephfs_data 128


ceph fs new mycephfs cephfs_metadata cephfs_data


ceph fs ls

ceph orch apply mds mycephfs --placement="ceph01,ceph02,ceph03"



ceph auth get-or-create client.cephfsuser \
  mon 'allow r' \
  mds 'allow r' \
  osd 'allow rw pool=cephfs_data' \
  -o /etc/ceph/ceph.client.cephfsuser.keyring



ls -l /etc/ceph/ceph.client.cephfsuser.keyring

scp -r /etc/ceph/ceph.client.cephfsuser.keyring ceph02:/etc/ceph/


### below are the steps done on ceph02 #############


dnf install ceph-fuse


ceph-fuse -n client.cephfsuser --keyring /etc/ceph/ceph.client.cephfsuser.keyring /mnt/test






$$$$$$$$$ CRUSH MAP $$$$$$$$$$$$$$$$$$$$
ceph osd getcrushmap -o crush.map
crushtool -d crush.map -o crush.txt


vim crush.txt

rule replicate_across_hosts {                               ----> at the end of file
    ruleset 10
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 2 type host
    step emit
}






crushtool -c crush.txt -o newcrush.map
ceph osd setcrushmap -i newcrush.map


ceph osd pool create hostpool 64 64 replicated replicate_across_hosts


ceph osd pool ls



echo "hello ceph" > testfile
rados -p hostpool put test1 testfile
ceph osd map hostpool test1



ceph orch daemon stop osd.1
rados -p hostpool get test1 test1-out
cat test1-out


ceph orch daemon restart osd.1






############### rbd (RADOS Block Device) ##################



ceph osd pool create rbd 128
rbd pool init rbd


rbd create mydisk --size 1024 --pool rbd


rbd ls rbd



dnf list installed | grep kernel-modules-extra


sudo dnf install -y kernel-modules-extra



rbd map mydisk --pool rbd



mkfs.ext4 /dev/nbd0
mkdir /mnt/myrbd
mount /dev/nbd0 /mnt/myrbd


umount /mnt/myrbd
rbd device unmap /dev/nbd0



rbd showmapped

########### snapshots###############


rbd snap create rbd/mydisk@snap1

rbd snap ls rbd/mydisk



rbd snap protect rbd/mydisk@snap1

rbd clone rbd/mydisk@snap1 rbd/myclone




######### Delete the Snapshot ###############


rbd snap unprotect rbd/mydisk@snap1

rbd snap rm rbd/mydisk@snap1




########### rollback snaphot #############

umount /mnt/myrbd
rbd device unmap /dev/rbd0
rbd snap rollback rbd/mydisk@snap1


rbd map mydisk --pool rbd
mount /dev/rbd0 /mnt/myrbd




############# delete cephfs ##########

[ceph: root@ceph01 /]# ceph orch ps --daemon-type mds
NAME                        HOST    PORTS  STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION            IMAGE ID      CONTAINER ID
mds.mycephfs.ceph01.givrpv  ceph01         running (2h)     3m ago  12d    24.0M        -  16.2.10-275.el8cp  d7a74ab527fa  3fbf925e1cdf
mds.mycephfs.ceph02.hpsjwm  ceph02         running (2h)     3m ago  12d    26.6M        -  16.2.10-275.el8cp  d7a74ab527fa  1d5007de486e
mds.mycephfs.ceph03.aqerbx  ceph03         running (2h)     8m ago  12d    25.3M        -  16.2.10-275.el8cp  d7a74ab527fa  e5a19aaad18f
[ceph: root@ceph01 /]# ceph orch rm mds.mycephfs
Removed service mds.mycephfs
[ceph: root@ceph01 /]# ceph orch ps --daemon-type mds
NAME                        HOST    PORTS  STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION            IMAGE ID      CONTAINER ID
mds.mycephfs.ceph03.aqerbx  ceph03         running (2h)     9m ago  12d    25.3M        -  16.2.10-275.el8cp  d7a74ab527fa  e5a19aaad18f
[ceph: root@ceph01 /]# ceph fs fail mycephfs
mycephfs marked not joinable; MDS cannot join the cluster. All MDS ranks marked failed.
[ceph: root@ceph01 /]# ceph fs rm mycephfs --yes-i-really-mean-it
[ceph: root@ceph01 /]# ceph osd pool delete cephfs_data cephfs_data --yes-i-really-really-mean-it
pool 'cephfs_data' removed
[ceph: root@ceph01 /]# ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it
pool 'cephfs_metadata' removed
[ceph: root@ceph01 /]#





















